{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282df64a",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "## 5.1 - TITLE\n",
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80edd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rnd\n",
    "import scipy.stats as sts\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7885f31c",
   "metadata": {},
   "source": [
    "## 5.3 - Auxiliary Functions\n",
    "Purpose: assuming data from lognormal distribution to compute the MLE of $mu$ and $sigma^2$.\n",
    "Do not use the scipy.stats.lognorm.fit which computes in addition a location parameter.\n",
    "The $mu$ and $sigma^2$ estimates can be quite different then.\n",
    "Input: array with data.\n",
    "Output: return the MLE's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb8ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_norm(vY):\n",
    "    muhat = np.mean((vY))\n",
    "    sigma2hat = np.var((vY))\n",
    "   \n",
    "    return muhat,sigma2hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf0897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval_direct(vY,tau,alpha):\n",
    "    muhat,sigma2hat = mle_norm(vY)\n",
    "    \n",
    "    Qhat = sts.norm.ppf(tau,muhat,scale=np.sqrt(sigma2hat))\n",
    "    \n",
    "    n = len(vY)\n",
    "    \n",
    "    quantile_normal_distribution = sts.norm.ppf(1 - 0.5*alpha)\n",
    "    \n",
    "    QL = Qhat - quantile_normal_distribution*np.sqrt(sigma2hat)\n",
    "    QR = Qhat + quantile_normal_distribution*np.sqrt(sigma2hat)\n",
    "    \n",
    "    return (QL , QR , Qhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cfbbd6",
   "metadata": {},
   "source": [
    "The data file is demanddata1000x20.txt which has 1000 rows and 20 columns. Use the first column as 1000 demand data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce16dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exercise5():\n",
    "    cost = 2.95\n",
    "    price = 3.27\n",
    "    holding_cost = 0.07\n",
    "    alpha = 0.05 # significance\n",
    "    \n",
    "    filename = 'BakeryData.xlsx' # data file\n",
    "    data_file = pd.read_excel(filename)\n",
    "\n",
    "    vY = data_file.iloc[0:1004:7,2] # the demand data. Verander in [] 0 naar 0-6 voor de verschillende dagen\n",
    "    tau = (price-cost) / (price + holding_cost)\n",
    "    \n",
    "    QL,QR,Qhat = confidence_interval_direct(vY,tau,alpha)\n",
    "    \n",
    "    print('direct confidence interval for optimal order Q:')\n",
    "    print('(',QL, ',', QR,')')\n",
    "    print(Qhat)\n",
    "    print('klaar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56910341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct confidence interval for optimal order Q:\n",
      "( -2.2645240615042397 , 67.07618145194152 )\n",
      "32.405828695218645\n",
      "klaar\n"
     ]
    }
   ],
   "source": [
    "exercise5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b56be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d827ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
